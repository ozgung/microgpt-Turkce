{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ozgung/microgpt-Turkce/blob/main/microgpt_T%C3%BCrk%C3%A7e.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> GPT’yi saf, bağımlılıksız Python ile eğitmenin ve çıkarım yapmanın en “atomik” yolu.\\\n",
        "> Bu dosya algoritmanın tamamı.\\\n",
        "> Geri kalan her şey sadece verimlilik.\\\n",
        "> @karpathy\n",
        "\n",
        "200 satırda GPT eğiten ve çıkarım (inference) yapan microgpt projesinin Türkçe açıklamasına hoşgeldiniz. Aşağıda kodu aşama aşama inceleyeceğiz.\n",
        "\n",
        "İngilizce orijinal [Colab](https://colab.research.google.com/drive/1vyN5zo6rqUp_dYNbT4Yrco66zuWCZKoN?usp=sharing) ve\n",
        "[Blog yazısına](https://karpathy.github.io/2026/02/12/microgpt/) linklerden ulaşabilirsiniz.\n",
        "\n",
        "Bu proje en yalın haliyle Python kullanıyor ve dışarıdan hiçbir hazır kütüphane kullanmayacağız. Standart ktüpohaneden de yalnızca logaritma (`math.log`) ve üstel fonksiyonlar (`math.exp`) ile rastgele sayı modülünü (`random`) import ederek başlıyoruz. `os.path.exists` ve `urllib.request.urlretrieve` fonksiyonlarını ise sadece örnek veri dosyasını indirmek için kullanıyoruz.\n",
        "\n",
        "`random.seed(42)` rastgele fonksiyonların her zaman aynı sayıları aynı sırada üretmesini sağlayarak bize yardımcı olacak. Böylece hep aynı sonuçları göreceğiz. Bu sadece geliştirme sırasında hataları kolay bulabilmemiz için gerekli. Normalde bu satırı kaldırabiliriz.\n"
      ],
      "metadata": {
        "id": "Du-f9vMsbvNw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "WeNezXgVzgSo"
      },
      "outputs": [],
      "source": [
        "'''Translated to Turkish by ChatGPT from the orginal in English.'''\n",
        "'''İngilizce orijinalinden ChatGPT ile Türkçe'ye çevrilmiştir.'''\n",
        "'''https://colab.research.google.com/drive/1vyN5zo6rqUp_dYNbT4Yrco66zuWCZKoN?usp=sharing'''\n",
        "'''https://karpathy.github.io/2026/02/12/microgpt/'''\n",
        "\n",
        "import os       # os.path.exists\n",
        "import math     # math.log, math.exp\n",
        "import random   # random.seed, random.choices, random.gauss, random.shuffle\n",
        "random.seed(42) # Kaos içinde düzen olsun"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Eğitim Verisi\n",
        "Elimizde örnek olarak basit bir veriseti var. Linke tıklayarak içeriğini görebilirsiniz. Her satırda bir insan ismi olan bir liste bu. Aşağıda önce bu listeyi indirip `input.txt` dosyasına kaydediyoruz. Sonra dosyayı okuyup `docs` isimli listeyi oluşturuyoruz. 32033 elemanlı bu isim listesi eğitim için bizim veri setimiz olacak.\n",
        "\n",
        "Gerçek GPT'de bu `docs` listesinin her elemanı bir dökümanı, örneğin bir web sayfasındaki metinleri içerecek. Bu örnekte ise basit bir örnek olarak insan isimleri kullanılmış."
      ],
      "metadata": {
        "id": "HP_c5PEigMcj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Bir giriş veri kümesi `docs` olsun: dokümanlardan oluşan liste[str] (ör. isimlerden oluşan bir veri kümesi)\n",
        "if not os.path.exists('input.txt'):\n",
        "    import urllib.request\n",
        "    names_url = 'https://raw.githubusercontent.com/karpathy/makemore/refs/heads/master/names.txt'\n",
        "    urllib.request.urlretrieve(names_url, 'input.txt')\n",
        "docs = [l.strip() for l in open('input.txt').read().strip().split('\\n') if l.strip()] # dokümanlardan oluşan liste[str]\n",
        "random.shuffle(docs)\n",
        "print(f\"doküman sayısı: {len(docs)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Im-k47A1bcXU",
        "outputId": "33e2a1d5-5507-4faa-e66b-51acc7a57a21"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "doküman sayısı: 32033\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenleştirme\n",
        "Bu liste sözcüklerden yani harflerden oluşuyor. Fakat yapay öğrenme modeli için bize sayılar gerekiyor. Girdileri nasıl sayılara dönüştürebiliriz?\n",
        "Basit bir yöntem her harfe bir sayı atamak. İngiliz alfabesinde 26 harf var. Bir tane de dizinin başlangıcını tanımlayan özel bir sembol (token) tanımlıyoruz. Bu özel sembolünün ismi orijınal kodda BOS (Beginning of Sequence, Dizinin Başlangıcı) olarak isimlendirilmiş. Ben bunu başlangıç anlamında BAS olarak değiştirdim ki daha anlaşılır olsun. Model bu BAS tokenini ayraç olarak kullanmayı öğrenecek. Farkli isimleri (dökümanları) bu şekilde birbirinden ayıracağız. Örnek:\n",
        "`\n",
        "[BAS, e, m, m, a, BAS]\n",
        "`\n",
        "\n",
        "Burada **token** kavramına da değinelim. Dokumanı ayırdıgımız bu parçacıklara genelde token adı veriliyor. Biz basitçe her harf için bir token gibi bir eşleştirme yaptık. Her tokeni de bir tamsayı ile temsil ettik (token id). Gerçek GPT'de daha etkili bir yol olarak harf gruplarını tokena çeviriyorlar. GPT4'ün kullandığı token dönüştrücüsü [tiktoken](https://github.com/openai/tiktoken)'a buradan ulaşabilirsiniz."
      ],
      "metadata": {
        "id": "f_yuViHbh75V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dizgileri ayrık sembollere çeviren ve geri dönüştüren bir Tokenizer olsun\n",
        "uchars = sorted(set(''.join(docs))) # veri kümesindeki benzersiz karakterler token id’leri 0..n-1 olur\n",
        "BAS = len(uchars) # özel “Dizinin Başlangıcı” (BAS) token’ı için token id\n",
        "vocab_size = len(uchars) + 1 # toplam benzersiz token sayısı, +1 BAS içindir\n",
        "print(f\"sözlük boyutu: {vocab_size}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAjvOwSygK6f",
        "outputId": "4ee0caf8-7f11-4784-e0ab-05f90857bfff"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sözlük boyutu: 27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Otomatik Türev (AutoGrad)\n",
        "Geldik matematiksel olarak en yoğun ve belki de asıl işi yapan kısma. Normalde `pytorch`, `JAX` gibi kütüphanelerin en büyük katkısı da bu.\n",
        "\n",
        "Yapay Sinirsel Ağlar günümüzde Geri Yayılım (Backpropagation) algoritması ile eğitiliyor. Bütün ağı bir hesaplama grafiği olarak tanımlıyoruz. Bu ağ bizim girdilerimizi (bu örnekte tokenlerimizi) ve eğitilebilir model parametrelerini (ağırlık ve bias parametreleri gibi) alıyor. Model çıktısı ise yine bizim tanımladığımız bir kayıp fonksiyonuna (Loss Function) girerek tek bir sayıya (Loss) indirgeniyor. Eğitim sırasında bizim amacımız bu sayıyı (Loss) düşürmeye çalışmak. Bunu da model parametrelerini ufak ufak artırıp azaltarak yapıyoruz.\n",
        "\n",
        "Peki hangi parametreleri artırıp hangilerini azaltmamız gerektiğini nereden bileceğiz? İşte bunu **türev** bize bu ilişkiyi veriyor. Bir parametreyi \"bir tık\" artırdığımızda diğer bir değerinin ne kadar değişeceğinin ölçüsüne türev diyoruz. Gradyan ise türevin çok değişkenli haline verilen isim. Yani modeldeki bütün parametrelerin ne yönde değişmesi gerektiği **Gradyan vektörü** ile tanımlanıyor.\n",
        "\n",
        "Tüm bu türev hesaplamalarını kendimiz yapabilirdik. Neyse ki Otomatik Türev (AutoGrad) denilen mucizevi yöntem sayesinde bütün bu türev hesapları otomatik yapılabiliyor. Biz ise sadece modeli tanımlayan o hesaplama grafiğini tanımlıyoruz. `Pytorch` gibi kütüphaneler sayesinde bu model tanımlama işini de bildiğimizz python fonksiyonları yazarak yapabiliyoruz. Autograd arka planda bizim yaptığımız bütün matematiksel işlemlerin bir listesini tutuyor ve türevlerini hesaplıyor. Matematiğe hiç elimizi bile sürmeden matematiksel olarak hesaplaması gayet zor olacak şeyleri farkına bile varmadan yapabiliyoruz. İşte Yapay Zekanın son yıllarda bu kadar hızlı ilerlemesinin sebeplerinden biri de bu.\n",
        "\n",
        "Bu tür kütüphaneleri kullananlar bilecektir ki modelin yapacağı işlemleri `forward` adlı bir fonksiyon tanımlıyoruz. Bu fonksiyon, modelin (hesaplama grafiğinin) **ileri yönde** (forward) yani girdilerden çıktıya doğru hangi işlemleri yapacağını tanımlıyor.\n",
        "\n",
        "Geri yayılım algoritması ise geri yönde, yani kayıp değerinden başlayarak girdilere doğru geriye giderek her parametreye göre Lossun türevlerini oluşturuyor. Bu türevleri calculustaki **zincir kuralı**nı kullanarak yapıyor. Biz ise kodda `backward` fonksiyonunu çağırarak hesaplatıyoruz. İşte Autograd bu backward fonksiyonunu bizim için otomatik olarak yaratıyor.\n",
        "\n",
        "Fakat bu örnekte hazır hiçbir şey kullanmadığımız için Autograd'ı da kendimiz yazacağız. Korkmayın, sandığınızdan daha kolay.\n",
        "\n",
        "### Ev Yapımı Autograd kodunun açıklaması\n",
        "\n",
        "Öncelikle hesaplama grafiğimizdeki her bir düğümü temsilen `Value` sınıfı oluşturuyoruz. Bu sınıfın kendi içinde 4 darklı değer tutacak. `data` bu düğümün hesaplanan skaler değeri, `grad` lossun bu düğüme göre türevi, `_children` (çocuklar) bu düğümün hesaplanmasında kullanılan (ondan önceki katmandan gelen) düğümler, `_local_grads` ise bu düğümün çocuklarına göre yerel türevleri.\n",
        "\n",
        "Bu Value nesneleri ile yaptığımız her matematiksel işlemde yeni bir Value yaratıyoruz. Örneğin `c = a + b` işlemini tanımladığımızda c Value'sunu yaratıyoruz. c'nin çocukları a ve b Value objeleri. a ve b'nin .data değişkeninin 3 ve 4 oldupunu düşünelim. c.data bu durumda 7 olarak hesaplanıyor. Bu hesaplamayı yaptığımız + işlemi Value sınıfındaki __add__ fonksiyonunda ile tanımlanıyor. Biz bu __add__ fonksiyonunu aşağıda kendimiz tanımlıyoruz. Fakat tanımlarken sadece toplama işlemi yapmakla kalmıyoruz. Yeni bir c Value'su yaratıp onu döndürüyoruz. c.data elbette a + b toplamı yani 7 oluyor. c'nin çocukları olarak a ve b'yi kaydediyoruz. Ama en önemlisi toplama işlemi için için c'nin a'ya ve b'ye göre 'yerel' türevlerini tanımlayıp onları da c'ye keydediyoruz. Toplama için her iki türev de 1 (a + b'nin a'ya ve b'ye göre türevleri 1 ve 1).\n",
        "\n",
        "Bunu Value'ları kullanarak için yapmak istediğimiz bütün temel matematiksel işlemler için ayrı ayrı tanımlamamız gerekiyor. Bunların birleşiminden oluşan daha karmaşık işlemleri tekrar tanımlamamıza gerek yok. Buradaki GPT örneği için tablodaki tanımlar yetiyor:\n",
        "\n",
        "| İşlem | İleri | Yerel Gradyanlar |\n",
        "|-----------|----------|-------------------|\n",
        "| `a + b`   | $$a + b$$ | $$\\frac{\\partial}{\\partial a}=1,\\quad \\frac{\\partial}{\\partial b}=1$$ |\n",
        "| `a * b`   | $$a \\cdot b$$ | $$\\frac{\\partial}{\\partial a}=b,\\quad \\frac{\\partial}{\\partial b}=a$$ |\n",
        "| `a ** n`  | $$a^n$$ | $$\\frac{\\partial}{\\partial a}=n a^{n-1}$$ |\n",
        "| `log(a)`  | $$\\ln(a)$$ | $$\\frac{\\partial}{\\partial a}= \\frac{1}{a}$$ |\n",
        "| `exp(a)`  | $$e^a$$ | $$\\frac{\\partial}{\\partial a}= e^a$$ |\n",
        "| `relu(a)` | $$\\max(0,a)$$ | $$\\frac{\\partial}{\\partial a}= \\mathbf{1}_{a>0}$$ |\n",
        "\n",
        "#### `.backward` fonksiyonu:\n",
        "Dikkat ederseniz Value nesnelerini yaratırken (__init__) .grad değerlerini 0 olarak bıraktık. Bu degerleri yani gradyanları hesaplamak için backward fonksiyonunu kullanıyoruz. Örneğin loss.backward() şeklinde çağırdığımızda en sondaki loss Value'sundan başlayarak, sondan başa doğru, hesaplama grafiğindeki bütün Value düğümlerini geziyoruz ve grad değerlerini zincir kuralını kullanarak hesaplıyoruz.\n",
        "\n",
        "backward fonksiyonu içinde ilk yaptığımız iş bu tersine gezme sırasını oluşturmak. Buna tür bir sıralamaya tersine topolojik sıralama deniyor. Burada DFS (Depth First Search, Derinlik Öncelikli Arama) adlı klasik algoritmanın bir versiyonu kullanılarak topolojik sıralama bulunuyor (topo). DFS sırasında eğer bir düğümün bütün çocukları gezildiyse toplo listesine ekliyoruz.\n",
        "\n",
        "Loss'tan başlıyoruz ve onun kendine göre türevi 1. Yani en sonraki düğümün grad değeri 1 oluyor.\n",
        "\n",
        "Sonrasında `v in reversed(topo)` ile sondan başa doğru v düğümleri geziliyor. Burada yapılan işlem önemli.\n",
        "\n",
        "v'nin .grad değeri yerel gradyanlar ile çarpılarak çocuklara aktarılıyor.  Burada çocukların mevcut gradyanına v'den yeni gelen gradyanlar ekleniyor. Bunun sebebi ağda dallanmalar olabilmesi ve bir düğüme birden fazla koldan gradyan akabilmesi.\n",
        "\n",
        "backward tamamlandığında her düğümün grad değeri hesaplanmış oluyor. Bu grad degeri $\\frac{\\partial L}{\\partial v}$'yı, yani v.data'yı bir tık degistirirsek kayip degerinin ne yönde kaç tık değişeceğini gösteriyor.\n",
        "\n",
        "---\n",
        "Daha fazla ayrıntı istiyorsanız Karphathy'nin 2 buçuk saatlik [micrograd](https://www.youtube.com/watch?v=VMj-3S1tku0) videosunu izleyebilirsiniz.\n"
      ],
      "metadata": {
        "id": "FLgtVWSDqup8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Hesaplama grafiği üzerinden zincir kuralını özyinelemeli uygulayan Autograd olsun\n",
        "class Value:\n",
        "    __slots__ = ('data', 'grad', '_children', '_local_grads') # bellek kullanımı için Python optimizasyonu\n",
        "\n",
        "    def __init__(self, data, children=(), local_grads=()):\n",
        "        self.data = data                # ileri geçişte (forward) hesaplanan bu düğümün skaler değeri\n",
        "        self.grad = 0                   # geri geçişte (backward) kaybın bu düğüme göre türevi\n",
        "        self._children = children       # hesaplama grafiğinde bu düğümün çocukları\n",
        "        self._local_grads = local_grads # bu düğümün çocuklarına göre yerel türevleri\n",
        "\n",
        "    def __add__(self, other):\n",
        "        other = other if isinstance(other, Value) else Value(other)\n",
        "        return Value(self.data + other.data, (self, other), (1, 1))\n",
        "\n",
        "    def __mul__(self, other):\n",
        "        other = other if isinstance(other, Value) else Value(other)\n",
        "        return Value(self.data * other.data, (self, other), (other.data, self.data))\n",
        "\n",
        "    def __pow__(self, other): return Value(self.data**other, (self,), (other * self.data**(other-1),))\n",
        "    def log(self): return Value(math.log(self.data), (self,), (1/self.data,))\n",
        "    def exp(self): return Value(math.exp(self.data), (self,), (math.exp(self.data),))\n",
        "    def relu(self): return Value(max(0, self.data), (self,), (float(self.data > 0),))\n",
        "    def __neg__(self): return self * -1\n",
        "    def __radd__(self, other): return self + other\n",
        "    def __sub__(self, other): return self + (-other)\n",
        "    def __rsub__(self, other): return other + (-self)\n",
        "    def __rmul__(self, other): return self * other\n",
        "    def __truediv__(self, other): return self * other**-1\n",
        "    def __rtruediv__(self, other): return other * self**-1\n",
        "\n",
        "    def backward(self):\n",
        "        topo = []\n",
        "        visited = set()\n",
        "        def build_topo(v):\n",
        "            if v not in visited:\n",
        "                visited.add(v)\n",
        "                for child in v._children:\n",
        "                    build_topo(child)\n",
        "                topo.append(v) # not: bütün çocuklarını tamamladıktan sonra listeye ekleyebilirsin\n",
        "        build_topo(self)\n",
        "        self.grad = 1\n",
        "        for v in reversed(topo):\n",
        "            for child, local_grad in zip(v._children, v._local_grads):\n",
        "                child.grad += local_grad * v.grad\n"
      ],
      "metadata": {
        "id": "vcFzbLRahqqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Y4K8QnmnLl-e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Model bilgisini tutacak parametreleri başlat.\n",
        "n_embd = 16     # embedding boyutu\n",
        "n_head = 4      # attention head sayısı\n",
        "n_layer = 1     # katman sayısı\n",
        "block_size = 16 # maksimum dizi uzunluğu\n",
        "head_dim = n_embd // n_head # her head’in boyutu\n",
        "matrix = lambda nout, nin, std=0.08: [[Value(random.gauss(0, std)) for _ in range(nin)] for _ in range(nout)]\n",
        "state_dict = {'wte': matrix(vocab_size, n_embd), 'wpe': matrix(block_size, n_embd), 'lm_head': matrix(vocab_size, n_embd)}\n",
        "for i in range(n_layer):\n",
        "    state_dict[f'layer{i}.attn_wq'] = matrix(n_embd, n_embd)\n",
        "    state_dict[f'layer{i}.attn_wk'] = matrix(n_embd, n_embd)\n",
        "    state_dict[f'layer{i}.attn_wv'] = matrix(n_embd, n_embd)\n",
        "    state_dict[f'layer{i}.attn_wo'] = matrix(n_embd, n_embd)\n",
        "    state_dict[f'layer{i}.mlp_fc1'] = matrix(4 * n_embd, n_embd)\n",
        "    state_dict[f'layer{i}.mlp_fc2'] = matrix(n_embd, 4 * n_embd)\n",
        "params = [p for mat in state_dict.values() for row in mat for p in row] # parametreleri tek bir liste[Value] halinde düzleştir\n",
        "print(f\"parametre sayısı: {len(params)}\")\n",
        "\n",
        "# Model mimarisini tanımla: token dizisi + parametreler -> bir sonraki token için logits.\n",
        "# GPT-2’yi takip et, (GPT’ler arasında kutsanmış), küçük farklarla: layernorm -> rmsnorm, bias yok, GeLU -> ReLU\n",
        "def linear(x, w):\n",
        "    return [sum(wi * xi for wi, xi in zip(wo, x)) for wo in w]\n",
        "\n",
        "def softmax(logits):\n",
        "    max_val = max(val.data for val in logits)\n",
        "    exps = [(val - max_val).exp() for val in logits]\n",
        "    total = sum(exps)\n",
        "    return [e / total for e in exps]\n",
        "\n",
        "def rmsnorm(x):\n",
        "    ms = sum(xi * xi for xi in x) / len(x)\n",
        "    scale = (ms + 1e-5) ** -0.5\n",
        "    return [xi * scale for xi in x]\n",
        "\n",
        "def gpt(token_id, pos_id, keys, values):\n",
        "    tok_emb = state_dict['wte'][token_id] # token embedding’i\n",
        "    pos_emb = state_dict['wpe'][pos_id] # pozisyon embedding’i\n",
        "    x = [t + p for t, p in zip(tok_emb, pos_emb)] # token + pozisyon embedding’i\n",
        "    x = rmsnorm(x)\n",
        "\n",
        "    for li in range(n_layer):\n",
        "        # 1) Multi-head attention bloğu\n",
        "        x_residual = x\n",
        "        x = rmsnorm(x)\n",
        "        q = linear(x, state_dict[f'layer{li}.attn_wq'])\n",
        "        k = linear(x, state_dict[f'layer{li}.attn_wk'])\n",
        "        v = linear(x, state_dict[f'layer{li}.attn_wv'])\n",
        "        keys[li].append(k)\n",
        "        values[li].append(v)\n",
        "        x_attn = []\n",
        "        for h in range(n_head):\n",
        "            hs = h * head_dim\n",
        "            q_h = q[hs:hs+head_dim]\n",
        "            k_h = [ki[hs:hs+head_dim] for ki in keys[li]]\n",
        "            v_h = [vi[hs:hs+head_dim] for vi in values[li]]\n",
        "            attn_logits = [sum(q_h[j] * k_h[t][j] for j in range(head_dim)) / head_dim**0.5 for t in range(len(k_h))]\n",
        "            attn_weights = softmax(attn_logits)\n",
        "            head_out = [sum(attn_weights[t] * v_h[t][j] for t in range(len(v_h))) for j in range(head_dim)]\n",
        "            x_attn.extend(head_out)\n",
        "        x = linear(x_attn, state_dict[f'layer{li}.attn_wo'])\n",
        "        x = [a + b for a, b in zip(x, x_residual)]\n",
        "        # 2) MLP bloğu\n",
        "        x_residual = x\n",
        "        x = rmsnorm(x)\n",
        "        x = linear(x, state_dict[f'layer{li}.mlp_fc1'])\n",
        "        x = [xi.relu() for xi in x]\n",
        "        x = linear(x, state_dict[f'layer{li}.mlp_fc2'])\n",
        "        x = [a + b for a, b in zip(x, x_residual)]\n",
        "\n",
        "    logits = linear(x, state_dict['lm_head'])\n",
        "    return logits\n",
        "\n",
        "# Adam olsun, kutsanmış optimize edici ve tamponları\n",
        "learning_rate, beta1, beta2, eps_adam = 0.01, 0.85, 0.99, 1e-8\n",
        "m = [0.0] * len(params) # birinci moment tamponu\n",
        "v = [0.0] * len(params) # ikinci moment tamponu\n",
        "\n",
        "# Sırayla tekrarla\n",
        "num_steps = 1000 # eğitim adımı sayısı\n",
        "for step in range(num_steps):\n",
        "\n",
        "    # Tek bir doküman al, tokenize et, iki yanına BAS özel token’ını koy\n",
        "    doc = docs[step % len(docs)]\n",
        "    tokens = [BAS] + [uchars.index(ch) for ch in doc] + [BAS]\n",
        "    n = min(block_size, len(tokens) - 1)\n",
        "\n",
        "    # Token dizisini modelden geçir, kayba kadar tüm hesaplama grafiğini kur.\n",
        "    keys, values = [[] for _ in range(n_layer)], [[] for _ in range(n_layer)]\n",
        "    losses = []\n",
        "    for pos_id in range(n):\n",
        "        token_id, target_id = tokens[pos_id], tokens[pos_id + 1]\n",
        "        logits = gpt(token_id, pos_id, keys, values)\n",
        "        probs = softmax(logits)\n",
        "        loss_t = -probs[target_id].log()\n",
        "        losses.append(loss_t)\n",
        "    loss = (1 / n) * sum(losses) # doküman dizisi üzerindeki nihai ortalama kayıp. Düşük olsun.\n",
        "\n",
        "    # Kaybı geri yay, tüm model parametrelerine göre gradyanları hesapla.\n",
        "    loss.backward()\n",
        "\n",
        "    # Adam güncellemesi: gradyanlara göre model parametrelerini güncelle.\n",
        "    lr_t = learning_rate * (1 - step / num_steps) # doğrusal öğrenme oranı azaltımı\n",
        "    for i, p in enumerate(params):\n",
        "        m[i] = beta1 * m[i] + (1 - beta1) * p.grad\n",
        "        v[i] = beta2 * v[i] + (1 - beta2) * p.grad ** 2\n",
        "        m_hat = m[i] / (1 - beta1 ** (step + 1))\n",
        "        v_hat = v[i] / (1 - beta2 ** (step + 1))\n",
        "        p.data -= lr_t * m_hat / (v_hat ** 0.5 + eps_adam)\n",
        "        p.grad = 0\n",
        "\n",
        "    print(f\"adım {step+1:4d} / {num_steps:4d} | kayıp {loss.data:.4f}\")\n",
        "\n",
        "# Çıkarım (Inference): model bize geri “gevezelesin”\n",
        "temperature = 0.5 # (0, 1] aralığında, üretilen metnin “yaratıcılığını” kontrol eder, düşükten yükseğe\n",
        "print(\"\\n--- çıkarım (yeni, halüsinasyon isimler) ---\")\n",
        "for sample_idx in range(20):\n",
        "    keys, values = [[] for _ in range(n_layer)], [[] for _ in range(n_layer)]\n",
        "    token_id = BAS\n",
        "    sample = []\n",
        "    for pos_id in range(block_size):\n",
        "        logits = gpt(token_id, pos_id, keys, values)\n",
        "        probs = softmax([l / temperature for l in logits])\n",
        "        token_id = random.choices(range(vocab_size), weights=[p.data for p in probs])[0]\n",
        "        if token_id == BAS:\n",
        "            break\n",
        "        sample.append(uchars[token_id])\n",
        "    print(f\"örnek {sample_idx+1:2d}: {''.join(sample)}\")"
      ],
      "metadata": {
        "id": "j66pvQz4rQYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "arsgkcFPzhqn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}